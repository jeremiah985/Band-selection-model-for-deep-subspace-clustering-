{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from post_clustering import spectral_clustering, acc, nmi\n",
    "import scipy.io as sio\n",
    "import math\n",
    "\n",
    "\n",
    "class Conv2dSamePad(nn.Module):\n",
    "    \"\"\"\n",
    "    在 Conv2d 中实现 TensorFlow 的 'SAME' 填充模式。\n",
    "    当需要填充的像素数是奇数（例如 `m`）时，TensorFlow 会在右边多填充一列，\n",
    "    或在底部多填充一行。而 PyTorch 会在两边各填充 `m+1` 个像素，\n",
    "    即 PyTorch 总是在两边进行填充。因此，我们可以在调用 Conv2d 模块之前，\n",
    "    以 TensorFlow 的方式对张量进行填充。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(Conv2dSamePad, self).__init__()\n",
    "        self.kernel_size = kernel_size if type(kernel_size) in [list, tuple] else [kernel_size, kernel_size]\n",
    "        self.stride = stride if type(stride) in [list, tuple] else [stride, stride]\n",
    "   #问题可能出在这里\n",
    "    def forward(self, x):\n",
    "        in_height = x.size(2)#输入张量的高度\n",
    "        in_width = x.size(3)#宽度\n",
    "        \n",
    "        out_height = math.ceil(float(in_height) / float(self.stride[0]))\n",
    "        #经过卷积操作后输出的高度\n",
    "        out_width = math.ceil(float(in_width) / float(self.stride[1]))\n",
    "        pad_along_height = ((out_height - 1) * self.stride[0] + self.kernel_size[0] - in_height)\n",
    "        pad_along_width = ((out_width - 1) * self.stride[1] + self.kernel_size[1] - in_width)\n",
    "        pad_top = math.floor(pad_along_height / 2)\n",
    "        pad_left = math.floor(pad_along_width / 2)\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        pad_right = pad_along_width - pad_left\n",
    "        \n",
    "        return F.pad(x, [pad_left, pad_right, pad_top, pad_bottom], 'constant', 0)\n",
    "\n",
    "\n",
    "class ConvTranspose2dSamePad(nn.Module):\n",
    "    \"\"\"\n",
    "    This module implements the \"SAME\" padding mode for ConvTranspose2d as in Tensorflow.\n",
    "    A tensor with width w_in, feed it to ConvTranspose2d(ci, co, kernel, stride), the width of output tensor T_nopad:\n",
    "        w_nopad = (w_in - 1) * stride + kernel\n",
    "    If we use padding, i.e., ConvTranspose2d(ci, co, kernel, stride, padding, output_padding), the width of T_pad:\n",
    "        w_pad = (w_in - 1) * stride + kernel - (2*padding - output_padding) = w_nopad - (2*padding - output_padding)\n",
    "    Yes, in ConvTranspose2d, more padding, the resulting tensor is smaller, i.e., the padding is actually deleting row/col.\n",
    "    If `pad`=(2*padding - output_padding) is odd, Pytorch deletes more columns in the left, i.e., the first ceil(pad/2) and\n",
    "    last `pad - ceil(pad/2)` columns of T_nopad are deleted to get T_pad.\n",
    "    In contrast, Tensorflow deletes more columns in the right, i.e., the first floor(pad/2) and last `pad - floor(pad/2)`\n",
    "    columns are deleted.\n",
    "    For the height, Pytorch deletes more rows at top, while Tensorflow at bottom.\n",
    "    In practice, we usually want `w_pad = w_in * stride`, i.e., the \"SAME\" padding mode in Tensorflow,\n",
    "    so the number of columns to delete:\n",
    "        pad = 2*padding - output_padding = kernel - stride\n",
    "    We can solve the above equation and get:\n",
    "        padding = ceil((kernel - stride)/2), and\n",
    "        output_padding = 2*padding - (kernel - stride) which is either 1 or 0.\n",
    "    But to get the same result with Tensorflow, we should delete values by ourselves instead of using padding and\n",
    "    output_padding in ConvTranspose2d.\n",
    "    To get there, we check the following conditions:\n",
    "    If pad = kernel - stride is even, we can directly set padding=pad/2 and output_padding=0 in ConvTranspose2d.\n",
    "    If pad = kernel - stride is odd, we can use ConvTranspose2d to get T_nopad, and then delete `pad` rows/columns by\n",
    "    ourselves; or we can use ConvTranspose2d to delete `pad - 1` by setting `padding=(pad - 1) / 2` and `ouput_padding=0`\n",
    "    and then delete the last row/column of the resulting tensor by ourselves.\n",
    "    Here we implement the former case.\n",
    "    This module should be called after the ConvTranspose2d module with shared kernel_size and stride values.\n",
    "    And this module can only output a tensor with shape `stride * size_input`.\n",
    "    A more flexible module can be found in `yaleb.py` which can output arbitrary size as specified.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(ConvTranspose2dSamePad, self).__init__()\n",
    "        self.kernel_size = kernel_size if type(kernel_size) in [list, tuple] else [kernel_size, kernel_size]\n",
    "        self.stride = stride if type(stride) in [list, tuple] else [stride, stride]\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_height = x.size(2)\n",
    "        in_width = x.size(3)\n",
    "        pad_height = self.kernel_size[0] - self.stride[0]\n",
    "        pad_width = self.kernel_size[1] - self.stride[1]\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "        return x[:, :, pad_top:in_height - pad_bottom, pad_left: in_width - pad_right]\n",
    "\n",
    "\n",
    "class ConvAE(nn.Module):\n",
    "    def __init__(self, channels, kernels):\n",
    "        \"\"\"\n",
    "        :param channels: a list containing all channels including the input image channel (1 for gray, 3 for RGB)\n",
    "        :param kernels:  a list containing all kernel sizes, it should satisfy: len(kernels) = len(channels) - 1.\n",
    "        \"\"\"\n",
    "        super(ConvAE, self).__init__()\n",
    "        assert isinstance(channels, list) and isinstance(kernels, list)\n",
    "        self.encoder = nn.Sequential()\n",
    "        for i in range(1, len(channels)):\n",
    "            #  Each layer will divide the size of feature map by 2\n",
    "           \n",
    "            self.encoder.add_module('pad%d' % i, Conv2dSamePad(kernels[i - 1], 2))\n",
    "            \n",
    "            self.encoder.add_module('conv%d' % i,\n",
    "                                    nn.Conv2d(channels[i - 1], channels[i], kernel_size=kernels[i - 1], stride=2))\n",
    "            \n",
    "            self.encoder.add_module('relu%d' % i, nn.ReLU(True))\n",
    "        \n",
    "        self.decoder = nn.Sequential()\n",
    "        channels = list(reversed(channels))\n",
    "        kernels = list(reversed(kernels))\n",
    "        for i in range(len(channels) - 1):#代码执行到这里就出问题了\n",
    "            # Each layer will double the size of feature map\n",
    "            self.decoder.add_module('deconv%d' % (i + 1),#已经知道的是这个参数的选择会极大的影响我们后续输入输出维度是否匹配\n",
    "                                    nn.ConvTranspose2d(channels[i], channels[i + 1], kernel_size=kernels[i], stride=2))\n",
    "            self.decoder.add_module('padd%d' % i, ConvTranspose2dSamePad(kernels[i], 2))\n",
    "            self.decoder.add_module('relud%d' % i, nn.ReLU(True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        y = self.decoder(h)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SelfExpression(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(SelfExpression, self).__init__()\n",
    "        self.Coefficient = nn.Parameter(1.0e-8 * torch.ones(n, n, dtype=torch.float32), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):  # shape=[n, d]\n",
    "        y = torch.matmul(self.Coefficient, x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DSCNet(nn.Module):\n",
    "    def __init__(self, channels, kernels, num_sample):\n",
    "        super(DSCNet, self).__init__()\n",
    "        self.n = num_sample\n",
    "        self.ae = ConvAE(channels, kernels)\n",
    "        self.self_expression = SelfExpression(self.n)\n",
    "\n",
    "    def forward(self, x):  # shape=[n, c, w, h]\n",
    "        z = self.ae.encoder(x)\n",
    "\n",
    "        # self expression layer, reshape to vectors, multiply Coefficient, then reshape back\n",
    "        shape = z.shape\n",
    "        z = z.view(self.n, -1)  # shape=[n, d]\n",
    "        z_recon = self.self_expression(z)  # shape=[n, d]\n",
    "        z_recon_reshape = z_recon.view(shape)\n",
    "\n",
    "        x_recon = self.ae.decoder(z_recon_reshape)  # shape=[n, c, w, h]\n",
    "        #x_recon = x_recon[:, :, :145, :145]  # 假设你需要裁剪到 (145, 145)\n",
    "        return x_recon, z, z_recon\n",
    "\n",
    "    def loss_fn(self, x, x_recon, z, z_recon, weight_coef, weight_selfExp):\n",
    "        loss_ae = F.mse_loss(x_recon, x, reduction='sum')\n",
    "        loss_coef = torch.sum(torch.pow(self.self_expression.Coefficient, 2))\n",
    "        loss_selfExp = F.mse_loss(z_recon, z, reduction='sum')\n",
    "        loss = loss_ae + weight_coef * loss_coef + weight_selfExp * loss_selfExp\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train(model,  # type: DSCNet\n",
    "          x, y, epochs, lr=1e-3, weight_coef=1.0, weight_selfExp=150, device='cuda',\n",
    "          alpha=0.04, dim_subspace=12, ro=8, show=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "    x = x.to(device)\n",
    "    if isinstance(y, torch.Tensor):\n",
    "        y = y.to('cpu').numpy()\n",
    "    K = len(np.unique(y))\n",
    "    for epoch in range(epochs):\n",
    "        x_recon, z, z_recon = model(x)\n",
    "        loss = model.loss_fn(x, x_recon, z, z_recon, weight_coef=weight_coef, weight_selfExp=weight_selfExp)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % show == 0 or epoch == epochs - 1:\n",
    "            C = model.self_expression.Coefficient.detach().to('cpu').numpy()\n",
    "            y_pred = spectral_clustering(C, K, dim_subspace, alpha, ro)\n",
    "            print('Epoch %02d: loss=%.4f, acc=%.4f, nmi=%.4f' %\n",
    "                  (epoch, loss.item() / y_pred.shape[0], acc(y, y_pred), nmi(y, y_pred)))\n",
    "\n",
    "\n",
    "# 7. 定义训练函数\n",
    "def train_with_band_selection(model, x, y, epochs, lr=1e-3, weight_coef=1.0, weight_selfExp=150, device='cuda',\n",
    "                              alpha=0.04, dim_subspace=12, ro=8, show=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "    x = x.to(device)\n",
    "    if isinstance(y, torch.Tensor):\n",
    "        y = y.to('cpu').numpy()\n",
    "    K = len(np.unique(y))\n",
    "\n",
    "    selected_bands = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        x_recon, z, z_recon = model(x)\n",
    "        loss = model.loss_fn(x, x_recon, z, z_recon, weight_coef=weight_coef, weight_selfExp=weight_selfExp)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % show == 0 or epoch == epochs - 1:\n",
    "            C = model.self_expression.Coefficient.detach().to('cpu').numpy()\n",
    "            y_pred = spectral_clustering(C, K, dim_subspace, alpha, ro)\n",
    "            print('Epoch %02d: loss=%.4f, acc=%.4f, nmi=%.4f' %\n",
    "                  (epoch, loss.item() / y_pred.shape[0], acc(y, y_pred), nmi(y, y_pred)))\n",
    "\n",
    "            # 选择波段\n",
    "            band_indices = np.argmax(np.abs(C), axis=0)\n",
    "            selected_bands.append(band_indices)\n",
    "\n",
    "    return selected_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels的初始维度： (1, 21025)\n",
      "x的形状： (200, 1, 145, 145)\n",
      "y的形状： (21025,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\32148\\AppData\\Local\\Temp\\ipykernel_1452\\1570276676.py:157: UserWarning: Using a target size (torch.Size([200, 1, 145, 145])) that is different to the input size (torch.Size([200, 1, 146, 146])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_ae = F.mse_loss(x_recon, x, reduction='sum')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (146) must match the size of tensor b (145) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\vscode\\HSI2\\DSC-Net\\最终实验版本.ipynb 单元格 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m dscnet \u001b[39m=\u001b[39m DSCNet(num_sample\u001b[39m=\u001b[39mnum_sample, channels\u001b[39m=\u001b[39mchannels, kernels\u001b[39m=\u001b[39mkernels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m dscnet\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m selected_bands \u001b[39m=\u001b[39m train_with_band_selection(dscnet, x, y, epochs\u001b[39m=\u001b[39;49mepochs, weight_coef\u001b[39m=\u001b[39;49mweight_coef, weight_selfExp\u001b[39m=\u001b[39;49mweight_selfExp,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                                                alpha\u001b[39m=\u001b[39;49malpha, dim_subspace\u001b[39m=\u001b[39;49mdim_subspace, ro\u001b[39m=\u001b[39;49mro, show\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# 输出最终选择的波段\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinal selected bands:\u001b[39m\u001b[39m\"\u001b[39m, selected_bands[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32md:\\vscode\\HSI2\\DSC-Net\\最终实验版本.ipynb 单元格 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m     x_recon, z, z_recon \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m     loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mloss_fn(x, x_recon, z, z_recon, weight_coef\u001b[39m=\u001b[39;49mweight_coef, weight_selfExp\u001b[39m=\u001b[39;49mweight_selfExp)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=204'>205</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32md:\\vscode\\HSI2\\DSC-Net\\最终实验版本.ipynb 单元格 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(\u001b[39mself\u001b[39m, x, x_recon, z, z_recon, weight_coef, weight_selfExp):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     loss_ae \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmse_loss(x_recon, x, reduction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m     loss_coef \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mpow(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_expression\u001b[39m.\u001b[39mCoefficient, \u001b[39m2\u001b[39m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%9C%80%E7%BB%88%E5%AE%9E%E9%AA%8C%E7%89%88%E6%9C%AC.ipynb#W1sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     loss_selfExp \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(z_recon, z, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:3365\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3362\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3363\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3365\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[0;32m   3366\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (146) must match the size of tensor b (145) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# 8. 主程序\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载Indian Pines数据集\n",
    "data_mat = sio.loadmat('indianpines_dataset.mat')\n",
    "labels_mat = sio.loadmat('indianpines_gt.mat')\n",
    "\n",
    "data = data_mat['pixels']\n",
    "labels = labels_mat['pixels']\n",
    "\n",
    "print(\"labels的初始维度：\",labels.shape)\n",
    "# 交换标签的第一维度和第二维度\n",
    "labels = np.transpose(labels, (1, 0))\n",
    "x, y = data.reshape((-1, 1, 145, 145)), labels\n",
    "y = np.squeeze(y)  # y in [0, 1, ..., K-1]\n",
    "\n",
    "print(\"x的形状：\",x.shape)\n",
    "print(\"y的形状：\",y.shape)\n",
    "\n",
    "# 网络和优化参数\n",
    "num_sample = x.shape[0]\n",
    "channels = [1, 20]  #每一层的通道数 可以调整\n",
    "kernels = [5]  #卷积核的大小 可以调整\n",
    "epochs = 50 # 训练的轮数\n",
    "weight_coef = 1.0# 自编码的权重\n",
    "weight_selfExp = 150# 自表达权重\n",
    "\n",
    "# 聚类后处理参数\n",
    "alpha = 0.04  # C的阈值\n",
    "dim_subspace = 12  # 每个子空间的维度\n",
    "ro = 8  #\n",
    "\n",
    "# 构建并训练模型\n",
    "dscnet = DSCNet(num_sample=num_sample, channels=channels, kernels=kernels)\n",
    "dscnet.to(device)\n",
    "\n",
    "selected_bands = train_with_band_selection(dscnet, x, y, epochs=epochs, weight_coef=weight_coef, weight_selfExp=weight_selfExp,\n",
    "                                               alpha=alpha, dim_subspace=dim_subspace, ro=ro, show=10, device=device)\n",
    "\n",
    "# 输出最终选择的波段\n",
    "print(\"Final selected bands:\", selected_bands[-1])\n",
    "\n",
    "    # 保存模型\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "torch.save(dscnet.state_dict(), 'results/dscnet_indian_pines.ckp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型的卷积层中进行适当的填充\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
