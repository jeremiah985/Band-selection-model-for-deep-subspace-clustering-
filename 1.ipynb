{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\vscode\\HSI2\\DSC-Net\\1.ipynb 单元格 1\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/1.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m \u001b[39m#激活函数\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/1.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/1.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpost_clustering\u001b[39;00m \u001b[39mimport\u001b[39;00m spectral_clustering, acc, nmi \u001b[39m#谱聚类\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/1.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msio\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/1.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n",
      "File \u001b[1;32md:\\vscode\\HSI2\\DSC-Net\\post_clustering.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m cluster\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m svds\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m normalize\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\__init__.py:84\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[39m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[39m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[39m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[39m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m         __check_build,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         _distributor_init,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     )\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     85\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     87\u001b[0m     __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_metadata_requests\u001b[39;00m \u001b[39mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bunch\u001b[39;00m \u001b[39mimport\u001b[39;00m Bunch\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_chunking\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     14\u001b[0m \u001b[39m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_chunking.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     12\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     17\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mInvalidParameterError\u001b[39;00m(\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m     18\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config \u001b[39mas\u001b[39;00m _get_config\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_array_api\u001b[39;00m \u001b[39mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_isfinite\u001b[39;00m \u001b[39mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_version\n\u001b[0;32m     13\u001b[0m _NUMPY_NAMESPACE_NAMES \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marray_api_compat.numpy\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\fixes.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexternals\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_packaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m _get_threadpool_controller\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\stats\\__init__.py:611\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    609\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m    610\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m--> 611\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    612\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    613\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_morestats\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1507\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1479\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1615\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim #优化\n",
    "import torch.nn.functional as F #激活函数\n",
    "import numpy as np\n",
    "from post_clustering import spectral_clustering, acc, nmi #谱聚类\n",
    "import scipy.io as sio\n",
    "import math\n",
    "\n",
    "class Conv2dSamePad(nn.Module):\n",
    "    \n",
    "    # 实现tensorflow中same的填充方式\n",
    "    # 在卷积操作后保持输入和输出的空间尽可能相同\n",
    "    # self代指类的实例本身\n",
    "    def __init__(self, kernel_size, stride):#卷积核的大小，步幅的大小\n",
    "        super(Conv2dSamePad, self).__init__()\n",
    "        self.kernel_size = kernel_size if type(kernel_size) in [list, tuple] else [kernel_size, kernel_size]\n",
    "        self.stride = stride if type(stride) in [list, tuple] else [stride, stride]\n",
    "    #计算输入张量通过卷积层后的输出张量\n",
    "    def forward(self, x):\n",
    "        in_height = x.size(2)#输入的张量高度\n",
    "        in_width = x.size(3)#宽度\n",
    "        out_height = math.ceil(float(in_height) / float(self.stride[0]))\n",
    "        #经过卷积这个操作后输出的高度\n",
    "        out_width = math.ceil(float(in_width) / float(self.stride[1]))\n",
    "        #卷积后输出的的宽度\n",
    "        #out_height 和 out_width: 计算经过卷积操作后，输出的张量高度和宽度。\n",
    "        #math.ceil 函数用于向上取整，\n",
    "        #这是因为卷积的步幅可能无法整除输入的维度，导致输出维度为小数。\n",
    "        pad_along_height = ((out_height - 1) * self.stride[0] + self.kernel_size[0] - in_height)\n",
    "        pad_along_width = ((out_width - 1) * self.stride[1] + self.kernel_size[1] - in_width)\n",
    "        #pad_along_height 和 pad_along_width: 计算在高度和宽度方向上需要填充的像素数。\n",
    "        #这是为了确保输出的尺寸与输入的尺寸尽可能相同。\n",
    "        pad_top = math.floor(pad_along_height / 2)\n",
    "        pad_left = math.floor(pad_along_width / 2)\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        pad_right = pad_along_width - pad_left\n",
    "        #pad_top 和 pad_left: 在高度和宽度方向上的上方和左侧需要填充的像素数。\n",
    "        #math.floor 用于向下取整。\n",
    "        #pad_bottom 和 pad_right: 在高度和宽度方向上的下方和右侧需要填充的像素数，通过减法计算得到。\n",
    "        return F.pad(x, [pad_left, pad_right, pad_top, pad_bottom], 'constant', 0)\n",
    "        #F.pad: 使用 PyTorch 的 pad 函数对输入张量 x 进行填充，按照计算得到的填充量 [pad_left, pad_right, pad_top, pad_bottom] \n",
    "        #在四个方向上进行填充。填充值为 0（即填充的区域为黑色或空白）\n",
    "        #填充顺序不能变，传参顺序\n",
    "class ConvTranspose2dSamePad(nn.Module):\n",
    "    # 对反卷积（输出尺寸大于目标尺寸的情况）操作的结果进行裁剪的过程\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(ConvTranspose2dSamePad, self).__init__()\n",
    "        self.kernel_size = kernel_size if type(kernel_size) in [list, tuple] else [kernel_size, kernel_size]\n",
    "        self.stride = stride if type(stride) in [list, tuple] else [stride, stride]\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_height = x.size(2)\n",
    "        in_width = x.size(3)\n",
    "        pad_height = self.kernel_size[0] - self.stride[0]\n",
    "        pad_width = self.kernel_size[1] - self.stride[1]\n",
    "        #高度和宽度上需要裁剪的像素数\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "        return x[:, :, pad_top:in_height - pad_bottom, pad_left: in_width - pad_right]\n",
    " # 在图片长度和宽度一样，同时卷积核的长度和宽度一样的情况下填充没有什么错误\n",
    " \n",
    "# 卷积自编码器：目的是实现特征提取和数据压缩\n",
    "class ConvAE(nn.Module):\n",
    "    def __init__(self, channels, kernels):\n",
    "        \"\"\"\n",
    "        channels列表：包含每一层的通道数，灰度：1,RGB:3\n",
    "        大小，它的长度应该比channels少一个，因为每一层卷积都需要一个卷积核。\n",
    "    \n",
    "        \"\"\"\n",
    "        super(ConvAE, self).__init__()\n",
    "        assert isinstance(channels, list) and isinstance(kernels, list)\n",
    "        #这里是一个断言调试，表示进入后续的一定要是一个list类型的变量\n",
    "        self.encoder = nn.Sequential()\n",
    "        #是 PyTorch 提供的一个容器类，我们这里实现的是一个编码器的过程\n",
    "        #它允许你将多个神经网络层（如卷积层、激活函数等）按照它们被添加的顺序组合在一起。\n",
    "        for i in range(1, len(channels)):\n",
    "            #  Each layer will divide the size of feature map by 2\n",
    "            #  步幅为2，特征（新生成的图像尺寸）通常会减半\n",
    "            self.encoder.add_module('pad%d' % i, Conv2dSamePad(kernels[i - 1], 2))\n",
    "            self.encoder.add_module('conv%d' % i,\n",
    "                                    nn.Conv2d(channels[i - 1], channels[i], kernel_size=kernels[i - 1], stride=2))\n",
    "            # 卷积层数，输入卷积层的通道数，输出到下一层的卷积通道数，卷积核的大小，步幅的大小\n",
    "            # \n",
    "            self.encoder.add_module('relu%d' % i, nn.ReLU(True))\n",
    "        #解码器\n",
    "        self.decoder = nn.Sequential()\n",
    "        channels = list(reversed(channels))\n",
    "        kernels = list(reversed(kernels))\n",
    "        #将通道和卷积核的列表反转，因为解码器的结构与编码器相反。\n",
    "        for i in range(len(channels) - 1):\n",
    "            # Each layer will double the size of feature map\n",
    "            self.decoder.add_module('deconv%d' % (i + 1),\n",
    "                                    nn.ConvTranspose2d(channels[i], channels[i + 1], kernel_size=kernels[i], stride=2))\n",
    "            self.decoder.add_module('padd%d' % i, ConvTranspose2dSamePad(kernels[i], 2))\n",
    "            self.decoder.add_module('relud%d' % i, nn.ReLU(True))\n",
    "#填充的目的->边缘可处理->\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        y = self.decoder(h)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SelfExpression(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(SelfExpression, self).__init__()\n",
    "        self.Coefficient = nn.Parameter(1.0e-8 * torch.ones(n, n, dtype=torch.float32), requires_grad=True)\n",
    "    def forward(self, x):  # shape=[n, d]\n",
    "        y = torch.matmul(self.Coefficient, x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DSCNet(nn.Module):\n",
    "    def __init__(self, channels, kernels, num_sample):\n",
    "        super(DSCNet, self).__init__()\n",
    "        self.n = num_sample\n",
    "        self.ae = ConvAE(channels, kernels)\n",
    "        self.self_expression = SelfExpression(self.n)\n",
    "\n",
    "    def forward(self, x):  # shape=[n, c, w, h]\n",
    "        z = self.ae.encoder(x)\n",
    "\n",
    "        # self expression layer, reshape to vectors, multiply Coefficient, then reshape back\n",
    "        shape = z.shape\n",
    "        z = z.view(self.n, -1)  # shape=[n, d]\n",
    "        z_recon = self.self_expression(z)  # shape=[n, d]\n",
    "        z_recon_reshape = z_recon.view(shape)\n",
    "\n",
    "        x_recon = self.ae.decoder(z_recon_reshape)  # shape=[n, c, w, h]\n",
    "        return x_recon, z, z_recon\n",
    "\n",
    "    def loss_fn(self, x, x_recon, z, z_recon, weight_coef, weight_selfExp):\n",
    "        loss_ae = F.mse_loss(x_recon, x, reduction='sum')\n",
    "        loss_coef = torch.sum(torch.pow(self.self_expression.Coefficient, 2))\n",
    "        loss_selfExp = F.mse_loss(z_recon, z, reduction='sum')\n",
    "        loss = loss_ae + weight_coef * loss_coef + weight_selfExp * loss_selfExp\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    '''\n",
    "    自编码损失 (loss_ae)：衡量重构输入与原始输入之间的差异。\n",
    "    系数矩阵的 L2 正则化损失 (loss_coef)：用于约束自表达系数矩阵，防止过拟合。\n",
    "    自表达损失 (loss_selfExp)：衡量通过自表达层处理后的特征与原始特征之间的差异。\n",
    "    '''\n",
    "\n",
    "\n",
    "def train(model,  # type: DSCNet\n",
    "          x, y, epochs, lr=1e-3, weight_coef=1.0, weight_selfExp=150, device='cuda',\n",
    "          alpha=0.04, dim_subspace=12, ro=8, show=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "    x = x.to(device)\n",
    "    if isinstance(y, torch.Tensor):\n",
    "        y = y.to('cpu').numpy()\n",
    "    K = len(np.unique(y))\n",
    "    for epoch in range(epochs):\n",
    "        x_recon, z, z_recon = model(x)\n",
    "        loss = model.loss_fn(x, x_recon, z, z_recon, weight_coef=weight_coef, weight_selfExp=weight_selfExp)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % show == 0 or epoch == epochs - 1:\n",
    "            C = model.self_expression.Coefficient.detach().to('cpu').numpy()\n",
    "            y_pred = spectral_clustering(C, K, dim_subspace, alpha, ro)\n",
    "            print('Epoch %02d: loss=%.4f, acc=%.4f, nmi=%.4f' %\n",
    "                  (epoch, loss.item() / y_pred.shape[0], acc(y, y_pred), nmi(y, y_pred)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    import warnings\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='DSCNet')\n",
    "    parser.add_argument('--db', default='coil20',\n",
    "                        choices=['coil20', 'coil100', 'orl', 'reuters10k', 'stl'])\n",
    "    parser.add_argument('--show-freq', default=10, type=int)\n",
    "    parser.add_argument('--ae-weights', default=None)\n",
    "    parser.add_argument('--save-dir', default='results')\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    db = args.db\n",
    "    if db == 'coil20':\n",
    "        # load data\n",
    "        data = sio.loadmat('datasets/COIL20.mat')\n",
    "        x, y = data['fea'].reshape((-1, 1, 32, 32)), data['gnd']\n",
    "        y = np.squeeze(y - 1)  # y in [0, 1, ..., K-1]\n",
    "\n",
    "        # network and optimization parameters\n",
    "        num_sample = x.shape[0]\n",
    "        channels = [1, 15]\n",
    "        kernels = [3]\n",
    "        epochs = 40\n",
    "        weight_coef = 1.0\n",
    "        weight_selfExp = 75\n",
    "\n",
    "        # post clustering parameters\n",
    "        alpha = 0.04  # threshold of C\n",
    "        dim_subspace = 12  # dimension of each subspace\n",
    "        ro = 8  #\n",
    "        warnings.warn(\"You can uncomment line#64 in post_clustering.py to get better result for this dataset!\")\n",
    "    elif db == 'coil100':\n",
    "        # load data\n",
    "        data = sio.loadmat('datasets/COIL100.mat')\n",
    "        x, y = data['fea'].reshape((-1, 1, 32, 32)), data['gnd']\n",
    "        y = np.squeeze(y - 1)  # y in [0, 1, ..., K-1]\n",
    "\n",
    "        # network and optimization parameters\n",
    "        num_sample = x.shape[0]\n",
    "        channels = [1, 50]\n",
    "        kernels = [5]\n",
    "        epochs = 120\n",
    "        weight_coef = 1.0\n",
    "        weight_selfExp = 15\n",
    "\n",
    "        # post clustering parameters\n",
    "        alpha = 0.04  # threshold of C\n",
    "        dim_subspace = 12  # dimension of each subspace\n",
    "        ro = 8  #\n",
    "    elif db == 'orl':\n",
    "        # load data\n",
    "        data = sio.loadmat('datasets/ORL_32x32.mat')\n",
    "        x, y = data['fea'].reshape((-1, 1, 32, 32)), data['gnd']\n",
    "        y = np.squeeze(y - 1)  # y in [0, 1, ..., K-1]\n",
    "        # network and optimization parameters\n",
    "        num_sample = x.shape[0]\n",
    "        channels = [1, 3, 3, 5]\n",
    "        kernels = [3, 3, 3]\n",
    "        epochs = 700\n",
    "        weight_coef = 2.0\n",
    "        weight_selfExp = 0.2\n",
    "\n",
    "        # post clustering parameters\n",
    "        alpha = 0.2  # threshold of C\n",
    "        dim_subspace = 3  # dimension of each subspace\n",
    "        ro = 1  #\n",
    "\n",
    "    dscnet = DSCNet(num_sample=num_sample, channels=channels, kernels=kernels)\n",
    "    dscnet.to(device)\n",
    "\n",
    "    # load the pretrained weights which are provided by the original author in\n",
    "    # https://github.com/panji1990/Deep-subspace-clustering-networks\n",
    "    ae_state_dict = torch.load('pretrained_weights_original/%s.pkl' % db)\n",
    "    dscnet.ae.load_state_dict(ae_state_dict)\n",
    "    print(\"Pretrained ae weights are loaded successfully.\")\n",
    "\n",
    "    train(dscnet, x, y, epochs, weight_coef=weight_coef, weight_selfExp=weight_selfExp,\n",
    "          alpha=alpha, dim_subspace=dim_subspace, ro=ro, show=args.show_freq, device=device)\n",
    "    torch.save(dscnet.state_dict(), args.save_dir + '/%s-model.ckp' % args.db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最近的成果整理：\n",
    "如上：复现了一个基于pytorch的深度子空间聚类网络模型\n",
    "能够在人家的coil数据集上跑成功\n",
    "但是在自己的indian_pines却无法复现\n",
    "1，数据的维度和组织格式不同\n",
    "2，代码报错信息显示维度不匹配，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
