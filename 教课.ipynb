{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels的初始维度： (1, 21025)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ConvTranspose2dSamePad.__init__() missing 2 required positional arguments: 'kernel_size' and 'stride'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\vscode\\HSI2\\DSC-Net\\教课.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m     \u001b[39m# Save the model\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m     torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mdscnet_indian_pines.ckpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m run_on_indian_pines()\n",
      "\u001b[1;32md:\\vscode\\HSI2\\DSC-Net\\教课.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m \u001b[39m# Initialize model and move to device\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m model \u001b[39m=\u001b[39m DSCNet(channels\u001b[39m=\u001b[39;49mchannels, kernels\u001b[39m=\u001b[39;49mkernels, num_sample\u001b[39m=\u001b[39;49mnum_sample)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n",
      "\u001b[1;32md:\\vscode\\HSI2\\DSC-Net\\教课.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39msuper\u001b[39m(DSCNet, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn \u001b[39m=\u001b[39m num_sample\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mae \u001b[39m=\u001b[39m ConvAE(channels, kernels)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_expression \u001b[39m=\u001b[39m SelfExpression(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn)\n",
      "\u001b[1;32md:\\vscode\\HSI2\\DSC-Net\\教课.ipynb 单元格 1\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(channels) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     \u001b[39m# Each layer will double the size of feature map\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39madd_module(\u001b[39m'\u001b[39m\u001b[39mdeconv\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m                             nn\u001b[39m.\u001b[39mConvTranspose2d(channels[i], channels[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m], kernel_size\u001b[39m=\u001b[39mkernels[i], stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39madd_module(\u001b[39m'\u001b[39m\u001b[39mpadd\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m i, ConvTranspose2dSamePad(kernels[i], \u001b[39m2\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscode/HSI2/DSC-Net/%E6%95%99%E8%AF%BE.ipynb#W0sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39madd_module(\u001b[39m'\u001b[39m\u001b[39mrelud\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m i, nn\u001b[39m.\u001b[39mReLU(\u001b[39mTrue\u001b[39;00m))\n",
      "\u001b[1;31mTypeError\u001b[0m: ConvTranspose2dSamePad.__init__() missing 2 required positional arguments: 'kernel_size' and 'stride'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from post_clustering import spectral_clustering, acc, nmi\n",
    "import scipy.io as sio\n",
    "import math\n",
    "\n",
    "# Conv2dSamePad, ConvTranspose2dSamePad, ConvAE, SelfExpression classes remain unchanged\n",
    "\n",
    "class Conv2dSamePad(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(Conv2dSamePad, self).__init__()\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, (list, tuple)) else [kernel_size, kernel_size]\n",
    "        self.stride = stride if isinstance(stride, (list, tuple)) else [stride, stride]\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_height = x.size(2)\n",
    "        in_width = x.size(3)\n",
    "        out_height = math.ceil(float(in_height) / float(self.stride[0]))\n",
    "        out_width = math.ceil(float(in_width) / float(self.stride[1]))\n",
    "        pad_along_height = max((out_height - 1) * self.stride[0] + self.kernel_size[0] - in_height, 0)\n",
    "        pad_along_width = max((out_width - 1) * self.stride[1] + self.kernel_size[1] - in_width, 0)\n",
    "        pad_top = pad_along_height // 2\n",
    "        pad_left = pad_along_width // 2\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        pad_right = pad_along_width - pad_left\n",
    "\n",
    "        # Apply padding\n",
    "        return F.pad(x, [pad_left, pad_right, pad_top, pad_bottom], mode='constant', value=0)\n",
    "\n",
    "\n",
    "class ConvTranspose2dSamePad(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(ConvTranspose2dSamePad, self).__init__()\n",
    "        self.kernel_size = kernel_size if type(kernel_size) in [list, tuple] else [kernel_size, kernel_size]\n",
    "        self.stride = stride if type(stride) in [list, tuple] else [stride, stride]\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_height = x.size(2)\n",
    "        in_width = x.size(3)\n",
    "        pad_height = self.kernel_size[0] - self.stride[0]\n",
    "        pad_width = self.kernel_size[1] - self.stride[1]\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "        return x[:, :, pad_top:in_height - pad_bottom, pad_left: in_width - pad_right]\n",
    "\n",
    "\n",
    "\n",
    "class SelfExpression(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(SelfExpression, self).__init__()\n",
    "        self.Coefficient = nn.Parameter(1.0e-8 * torch.ones(n, n, dtype=torch.float32), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.matmul(self.Coefficient, x)\n",
    "        return y\n",
    "class ConvAE(nn.Module):\n",
    "    def __init__(self, channels, kernels):\n",
    "        \"\"\"\n",
    "        :param channels: a list containing all channels including the input image channel (1 for gray, 3 for RGB)\n",
    "        :param kernels:  a list containing all kernel sizes, it should satisfy: len(kernels) = len(channels) - 1.\n",
    "        \"\"\"\n",
    "        super(ConvAE, self).__init__()\n",
    "        assert isinstance(channels, list) and isinstance(kernels, list)\n",
    "        self.encoder = nn.Sequential()\n",
    "        for i in range(1, len(channels)):\n",
    "            #  Each layer will divide the size of feature map by 2\n",
    "            self.encoder.add_module('pad%d' % i, Conv2dSamePad(kernels[i - 1], 2))\n",
    "            self.encoder.add_module('conv%d' % i,\n",
    "                                    nn.Conv2d(channels[i - 1], channels[i], kernel_size=kernels[i - 1], stride=2))\n",
    "            self.encoder.add_module('relu%d' % i, nn.ReLU(True))\n",
    "\n",
    "        self.decoder = nn.Sequential()\n",
    "        channels = list(reversed(channels))\n",
    "        kernels = list(reversed(kernels))\n",
    "        for i in range(len(channels) - 1):\n",
    "            # Each layer will double the size of feature map\n",
    "            self.decoder.add_module('deconv%d' % (i + 1),\n",
    "                                    nn.ConvTranspose2d(channels[i], channels[i + 1], kernel_size=kernels[i], stride=2))\n",
    "            self.decoder.add_module('padd%d' % i, ConvTranspose2dSamePad(kernels[i], 2))\n",
    "            self.decoder.add_module('relud%d' % i, nn.ReLU(True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        y = self.decoder(h)\n",
    "        return y\n",
    "class DSCNet(nn.Module):\n",
    "    def __init__(self, channels, kernels, num_sample):\n",
    "        super(DSCNet, self).__init__()\n",
    "        self.n = num_sample\n",
    "        self.ae = ConvAE(channels, kernels)\n",
    "        self.self_expression = SelfExpression(self.n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.ae.encoder(x)\n",
    "        shape = z.shape\n",
    "        z = z.view(self.n, -1)\n",
    "        z_recon = self.self_expression(z)\n",
    "        z_recon_reshape = z_recon.view(shape)\n",
    "        x_recon = self.ae.decoder(z_recon_reshape)\n",
    "        z_recon_reshape = z_recon.view(shape)\n",
    "        \n",
    "\n",
    "# 在这里手动调整张量的大小\n",
    "        #x_recon = x_recon[:, :, :145, :145]  # 假设你需要裁剪到 (145, 145)\n",
    "\n",
    "        return x_recon, z, z_recon\n",
    "\n",
    "    def loss_fn(self, x, x_recon, z, z_recon, weight_coef, weight_selfExp):\n",
    "        loss_ae = F.mse_loss(x_recon, x, reduction='sum')\n",
    "        loss_coef = torch.sum(torch.pow(self.self_expression.Coefficient, 2))\n",
    "        loss_selfExp = F.mse_loss(z_recon, z, reduction='sum')\n",
    "        loss = loss_ae + weight_coef * loss_coef + weight_selfExp * loss_selfExp\n",
    "        return loss\n",
    "\n",
    "def train(model, x, y, epochs, lr=1e-3, weight_coef=1.0, weight_selfExp=150, device='cuda',\n",
    "          alpha=0.04, dim_subspace=12, ro=8, show=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "    x = x.to(device)\n",
    "    if isinstance(y, torch.Tensor):\n",
    "        y = y.to('cpu').numpy()\n",
    "    K = len(np.unique(y))\n",
    "    for epoch in range(epochs):\n",
    "        x_recon, z, z_recon = model(x)\n",
    "        loss = model.loss_fn(x, x_recon, z, z_recon, weight_coef=weight_coef, weight_selfExp=weight_selfExp)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % show == 0 or epoch == epochs - 1:\n",
    "            C = model.self_expression.Coefficient.detach().to('cpu').numpy()\n",
    "            y_pred = spectral_clustering(C, K, dim_subspace, alpha, ro)\n",
    "            print('Epoch %02d: loss=%.4f, acc=%.4f, nmi=%.4f' %\n",
    "                  (epoch, loss.item() / y_pred.shape[0], acc(y, y_pred), nmi(y, y_pred)))\n",
    "\n",
    "def run_on_indian_pines():\n",
    "    # Load Indian Pines dataset\n",
    "    \n",
    "    # 加载Indian Pines数据集\n",
    "    data_mat = sio.loadmat('indianpines_dataset.mat')\n",
    "    labels_mat = sio.loadmat('indianpines_gt.mat')\n",
    "\n",
    "    data = data_mat['pixels']\n",
    "    labels = labels_mat['pixels']\n",
    "\n",
    "    print(\"labels的初始维度：\",labels.shape)\n",
    "    # 交换标签的第一维度和第二维度\n",
    "    labels = np.transpose(labels, (1, 0))\n",
    "    x, y = data.reshape((-1, 1, 145, 145)), labels\n",
    "    y = np.squeeze(y)  # y in [0, 1, ..., K-1]\n",
    "    # Network parameters\n",
    "\n",
    "    num_sample = x.shape[0]\n",
    "    channels = [1, 200]\n",
    "    kernels = [5]\n",
    "    epochs = 40\n",
    "    weight_coef = 1.0\n",
    "    weight_selfExp = 75\n",
    "    alpha = 0.04\n",
    "    dim_subspace = 12\n",
    "    ro = 8\n",
    "\n",
    "    # Initialize model and move to device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = DSCNet(channels=channels, kernels=kernels, num_sample=num_sample)\n",
    "    model.to(device)\n",
    "\n",
    "    # Train the model\n",
    "    train(model, x, y, epochs, weight_coef=weight_coef, weight_selfExp=weight_selfExp,\n",
    "          alpha=alpha, dim_subspace=dim_subspace, ro=ro, device=device)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'dscnet_indian_pines.ckpt')\n",
    "\n",
    "\n",
    "run_on_indian_pines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1000,99,-1):\n",
    "    if(555555%i==0):\n",
    "       print(i)\n",
    "       break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
